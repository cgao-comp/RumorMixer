{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Libs.Datasets.load_dataset import load_UPFD\n",
    "from torch_geometric import seed_everything\n",
    "\n",
    "seed_everything(3407)\n",
    "\n",
    "K = 3\n",
    "name = 'politifact'\n",
    "feature = 'spacy'\n",
    "batch_size = 32\n",
    "train_dataset, val_dataset, test_dataset, train_loader, val_loader, test_loader = load_UPFD(\n",
    "    name, feature, batch_size=batch_size, K=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = 8\n",
    "sample_g = train_dataset[sample_id]\n",
    "sample_g, sample_g.partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Dropout, MultiheadAttention\n",
    "from torch_geometric.nn import global_mean_pool, GATv2Conv, SAGEConv, GCNConv, global_max_pool\n",
    "import torch.nn as nn\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, conv, act=F.relu, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv = conv\n",
    "        self.act = act\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv(x, edge_index)\n",
    "        if self.act:\n",
    "            x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GraphEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, pe_dim, num_heads=8, dropout=0.0):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        self.conv = GNNLayer(\n",
    "            SAGEConv(in_channels + pe_dim, hidden_channels), dropout=dropout)\n",
    "        self.pe_norm = nn.BatchNorm1d(pe_dim)\n",
    "        self.pe_lin = Linear(pe_dim, pe_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            self.conv,\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, edge_index, pe):\n",
    "        pe_norm = self.pe_norm(pe)\n",
    "        # Combine node features with positional encoding\n",
    "        x = torch.cat((x, self.pe_lin(pe_norm)), dim=1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GraphEncoderwithoutPE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_heads=8, dropout=0.0):\n",
    "        super(GraphEncoderwithoutPE, self).__init__()\n",
    "        self.conv = GNNLayer(\n",
    "            SAGEConv(in_channels, hidden_channels), dropout=dropout)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            self.conv,\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            # nn.Linear(dim, hidden_dim),\n",
    "            # nn.Dropout(dropout),\n",
    "            nn.GELU(),\n",
    "            # nn.Dropout(dropout),\n",
    "            # nn.Linear(hidden_dim, dim),\n",
    "            # nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MixerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_partitions, token_dim, channel_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_mix = nn.Sequential(\n",
    "            nn.LayerNorm(num_features),\n",
    "            Rearrange('p d -> d p'),\n",
    "            FeedForward(num_partitions, token_dim, dropout),\n",
    "            Rearrange('d p -> p d'),\n",
    "        )\n",
    "        self.channel_mix = nn.Sequential(\n",
    "            # nn.LayerNorm(num_features),\n",
    "            FeedForward(num_features, channel_dim, dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.token_mix(x)\n",
    "        x = x + self.channel_mix(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLPMixer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_channels,\n",
    "                 num_partitions,\n",
    "                 num_layers=3,\n",
    "                 with_final_norm=True,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_partitions = num_partitions\n",
    "        self.with_final_norm = with_final_norm\n",
    "        self.mixer_blocks = nn.ModuleList(\n",
    "            [MixerBlock(hidden_channels, self.num_partitions, hidden_channels*4, hidden_channels//2, dropout=dropout) for _ in range(num_layers)])\n",
    "        if self.with_final_norm:\n",
    "            self.layer_norm = nn.LayerNorm(hidden_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for mixer_block in self.mixer_blocks:\n",
    "            x = mixer_block(x)\n",
    "        if self.with_final_norm:\n",
    "            x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionMixerBlock(nn.Module):\n",
    "    def __init__(self, num_features, num_partitions, token_dim, channel_dim, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_partitions = num_partitions\n",
    "        self.attention = MultiheadAttention(\n",
    "            embed_dim=num_features, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.token_mix = nn.Sequential(\n",
    "            Rearrange('p d -> d p'),\n",
    "            FeedForward(num_partitions, token_dim, dropout),\n",
    "            Rearrange('d p -> p d'),\n",
    "        )\n",
    "        self.channel_mix = FeedForward(num_features, channel_dim, dropout)\n",
    "        self.norm1 = nn.LayerNorm(num_features)\n",
    "        self.norm2 = nn.LayerNorm(num_features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention\n",
    "        x = x + self.dropout(self.attention(self.norm1(x),\n",
    "                             self.norm1(x), self.norm1(x))[0])\n",
    "        # Token mixing\n",
    "        x = x + self.token_mix(x)\n",
    "        # Channel mixing\n",
    "        x = x + self.channel_mix(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MHAMixer(nn.Module):\n",
    "    def __init__(self, hidden_channels, num_partitions, num_heads=8, num_layers=3, with_final_norm=True, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_partitions = num_partitions\n",
    "        self.with_final_norm = with_final_norm\n",
    "        self.mixer_blocks = nn.ModuleList([\n",
    "            AttentionMixerBlock(hidden_channels, num_partitions,\n",
    "                                hidden_channels*4, hidden_channels//2, num_heads, dropout)\n",
    "            for _ in range(num_layers)])\n",
    "        if with_final_norm:\n",
    "            self.layer_norm = nn.LayerzNorm(hidden_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for mixer_block in self.mixer_blocks:\n",
    "            x = mixer_block(x)\n",
    "        if self.with_final_norm:\n",
    "            x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_layers=2, with_final_activation=True, with_norm=False):\n",
    "        super().__init__()\n",
    "        hidden_channels = in_channels\n",
    "        layers = []\n",
    "        norms = []\n",
    "        for i in range(num_layers):\n",
    "            out_channels = hidden_channels if i < num_layers-1 else out_channels\n",
    "            layers.append(nn.Linear(\n",
    "                in_channels if i == 0 else hidden_channels, out_channels, bias=not with_norm))\n",
    "            if with_norm:\n",
    "                norms.append(nn.BatchNorm1d(out_channels))\n",
    "            else:\n",
    "                norms.append(Identity())\n",
    "            in_channels = out_channels\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norms = nn.ModuleList(norms)\n",
    "        self.activation = F.relu if with_final_activation else Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer, norm in zip(self.layers, self.norms):\n",
    "            x = layer(x)\n",
    "            x = norm(x)\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GraphMixerModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_partitions, pe_dim):\n",
    "        super(GraphMixerModel, self).__init__()\n",
    "        self.encoder = GraphEncoder(in_channels, hidden_channels, pe_dim)\n",
    "        self.mixer = MLPMixer(hidden_channels, num_partitions,\n",
    "                              num_layers=1, with_final_norm=False)\n",
    "        # self.mixer = MHAMixer(hidden_channels, num_partitions, num_layers=1)\n",
    "        self.pool = global_max_pool\n",
    "        # self.cls = Classifier(hidden_channels, out_channels)\n",
    "        self.cls = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # print(f\"Input: {batch}\")\n",
    "        # Process each partition\n",
    "        partition_embeddings = []\n",
    "        for partition in batch.partitions:\n",
    "            partition_x = self.encoder(\n",
    "                partition.x, partition.edge_index, partition.random_walk_pe)\n",
    "            # print(f\"GNN: {partition_x.shape}\")\n",
    "            # Global average pooling for each partitions\n",
    "            partition_embedding = self.pool(partition_x, partition.batch)\n",
    "            # print(f\"READOUT: {partition_embedding.shape}\")\n",
    "            partition_embeddings.append(partition_embedding)\n",
    "\n",
    "        # Stack embeddings and apply mixer\n",
    "        embeddings = torch.stack(partition_embeddings, dim=0).squeeze(1)\n",
    "        # # print(f\"Stacked: {embeddings.shape}\")\n",
    "        mixed_embeddings = self.mixer(embeddings)\n",
    "        # print(f\"Mixer: {mixed_embeddings.shape}\")\n",
    "        # # Generate the final graph-level representation by mean pooling\n",
    "        global_embedding = torch.mean(mixed_embeddings, dim=0)\n",
    "        # print(f\"Pooling: {global_embedding.shape}\")\n",
    "        # Final linear layer\n",
    "        out = self.cls(global_embedding)\n",
    "        # print(f\"Out: {out.shape}\")\n",
    "        return F.log_softmax(out, dim=-1)\n",
    "\n",
    "\n",
    "class NaiveGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(NaiveGNN, self).__init__()\n",
    "        self.conv = SAGEConv(in_channels, hidden_channels)\n",
    "        self.pool = global_max_pool\n",
    "        self.cls = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # print(f\"Input: {batch}\")\n",
    "        x = self.conv(data.x, data.edge_index)\n",
    "        # Generate the final graph-level representation by mean pooling\n",
    "        x = self.pool(x, data.batch)\n",
    "        # Final linear layer\n",
    "        out = self.cls(x)\n",
    "        # print(f\"Out: {out.shape}\")\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_partitions = K\n",
    "in_channels = train_dataset[0].num_node_features\n",
    "hidden_channels = 64\n",
    "out_channels = 2\n",
    "pe_dim = train_dataset[0].partitions[0].random_walk_pe.shape[1]\n",
    "model = GraphMixerModel(in_channels=in_channels, out_channels=out_channels, hidden_channels=hidden_channels,\n",
    "                        num_partitions=num_partitions, pe_dim=pe_dim).to(device)\n",
    "# model = NaiveGNN(\n",
    "#     in_channels=in_channels, out_channels=out_channels, hidden_channels=hidden_channels).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=100, T_mult=2, eta_min=1e-6)\n",
    "criterion = F.nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Use device:', device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "\n",
    "\n",
    "def train(dataset, model, optimizer, scheduler, criterion, device):\n",
    "    model.train()  # 将模型设置为训练模式\n",
    "    total_loss = []\n",
    "\n",
    "    for data in dataset:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()  # 清除旧的梯度\n",
    "        outputs = model(data).view(1, 2)  # 正向传播\n",
    "        loss = criterion(outputs, data.y)  # 计算损失\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新权重\n",
    "\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "    # 调整学习率\n",
    "    scheduler.step()\n",
    "\n",
    "    # 计算并返回平均损失\n",
    "    return sum(total_loss) / len(dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()  # 确保在此函数中不计算梯度\n",
    "def test(dataset, model, device):\n",
    "    model.eval()  # 将模型设置为评估模式\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    for data in dataset:\n",
    "        data = data.to(device)\n",
    "        outputs = model(data).argmax(dim=-1)\n",
    "        y_pred.append(outputs.cpu().numpy())  # 将预测结果转换为列表并添加到y_pred\n",
    "        y_true.extend(data.y.cpu().numpy())  # 将真实标签转换为列表并添加到y_true\n",
    "\n",
    "    # 计算F1分数\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "def run_exp(train_dataset, val_dataset, test_dataset, model, optimizer, scheduler, criterion, device, num_epochs=25, patience=10):\n",
    "\n",
    "    best_val_f1 = 0.0  # 最佳验证F1分数\n",
    "    patience_counter = 0  # 早停计数器\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # 训练阶段\n",
    "        loss = train(train_dataset, model, optimizer,\n",
    "                     scheduler, criterion, device)\n",
    "\n",
    "        # 评估阶段\n",
    "        train_f1 = test(train_dataset, model, device)\n",
    "        val_f1 = test(val_dataset, model, device)\n",
    "        test_f1 = test(test_dataset, model, device)\n",
    "\n",
    "        # 模型检查点保存逻辑\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(),\n",
    "                       f'./Output/Models/ckpt-{val_f1:.4f}.pt')\n",
    "            print(f\"Best model saved with F1: {val_f1:.4f}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # 早停逻辑\n",
    "        if patience_counter > patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "        # 打印性能指标\n",
    "        print(f'Epoch: {epoch:03d} | Loss: {loss:.4f} | Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f} | Test F1: {test_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exp(train_dataset, val_dataset, test_dataset, model, optimizer,\n",
    "        scheduler, criterion, device, num_epochs=10, patience=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
